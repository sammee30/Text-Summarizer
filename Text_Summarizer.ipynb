{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAGKq2CXZMMy"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the Library"
      ],
      "metadata": {
        "id": "YWIwE06PbghD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, load_from_disk\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "XAsvSnmqZbwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select the GPU"
      ],
      "metadata": {
        "id": "UOk78CFUbyGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n",
        ""
      ],
      "metadata": {
        "id": "EwkvZYRjbu7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the model Name and Initialize the Tokenizer and Model"
      ],
      "metadata": {
        "id": "MA82Z5i1b6UY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "\n",
        "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
      ],
      "metadata": {
        "id": "Z9b1y1EjZboU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility function designed to split a list into smaller, batch-sized chunks and calculates metric like ROUGE"
      ],
      "metadata": {
        "id": "Q_JxBvoUcH0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
        "    for i in range(0, len(list_of_elements), batch_size):\n",
        "        yield list_of_elements[i : i + batch_size]\n",
        "\n",
        "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n",
        "                               batch_size=16, device=device,\n",
        "                               column_text=\"article\",\n",
        "                               column_summary=\"highlights\"):\n",
        "\n",
        "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
        "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
        "\n",
        "    for article_batch, target_batch in tqdm(\n",
        "        zip(article_batches, target_batches), total=len(article_batches)):\n",
        "\n",
        "        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n",
        "                        padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
        "                         attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "                         length_penalty=0.8, num_beams=8, max_length=128)\n",
        "        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n",
        "\n",
        "        # Finally, we decode the generated texts,\n",
        "        # replace the <n> token, and add the decoded texts with the references to the metric.\n",
        "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
        "                                clean_up_tokenization_spaces=True)\n",
        "               for s in summaries]\n",
        "\n",
        "        decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n",
        "\n",
        "\n",
        "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
        "\n",
        "    #  Finally compute and return the ROUGE scores.\n",
        "    score = metric.compute()\n",
        "    return score"
      ],
      "metadata": {
        "id": "tABV-ZGqZblg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Dataset"
      ],
      "metadata": {
        "id": "1buqF0McdLZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/sammee30/Text-Summarizer/raw/main/data.zip\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "id": "6oytBnk4cxUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_samsum = load_from_disk('samsum_dataset')\n",
        "\n",
        "split_lengths = [len(dataset_samsum[split])for split in dataset_samsum]\n",
        "\n",
        "print(f\"Split lengths: {split_lengths}\")\n",
        "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
        "print(\"\\nDialogue:\")\n",
        "\n",
        "print(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
        "\n",
        "print(\"\\nSummary:\")\n",
        "\n",
        "print(dataset_samsum[\"test\"][0][\"summary\"])"
      ],
      "metadata": {
        "id": "DZGZNwalZbib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating PEGASUS on SAMSum"
      ],
      "metadata": {
        "id": "qjrdBX9bdTbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline('summarization', model = model_ckpt )\n",
        "\n",
        "pipe_out = pipe(dataset_samsum['test'][0]['dialogue'] )\n",
        "\n",
        "print(pipe_out)"
      ],
      "metadata": {
        "id": "fChPs_KYZbfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the Model on the Test Dataset"
      ],
      "metadata": {
        "id": "hw2pBD7Xdctw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "\n",
        "rouge_metric = load_metric('rouge')\n",
        "\n",
        "score = calculate_metric_on_test_ds(dataset_samsum['test'], rouge_metric, model_pegasus, tokenizer, column_text = 'dialogue', column_summary = 'summary', batch_size = 8)"
      ],
      "metadata": {
        "id": "HSW62H0GZbdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
        "\n",
        "pd.DataFrame(rouge_dict, index = ['pegasus'])"
      ],
      "metadata": {
        "id": "PVOBECyqZbaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dialogue_token_len = [len(tokenizer.encode(s) for s in dataset_samsum['train']['dialogue'] )]\n",
        "\n",
        "summary_token_len = [len(tokenizer.encode(s) for s in dataset_samsum['train']['summary'] )]\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey='C0' )\n",
        "axes[0].hist(dialogue_token_len, bins = 20, color = 'C0', edgecolor = 'C0' )\n",
        "axes[0].set_title(\"Dialogue Token Length\")\n",
        "axes[0].set_xlabel(\"Length\")\n",
        "axes[0].set_ylabel(\"Count\")\n",
        "\n",
        "axes[1].hist(summary_token_len, bins = 20, color = 'C0', edgecolor = 'C0' )\n",
        "axes[1].set_title(\"Summary Token Length\")\n",
        "axes[1].set_xlabel(\"Length\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A4X_exJWZbXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning"
      ],
      "metadata": {
        "id": "Ys__TsaldkAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing the Input"
      ],
      "metadata": {
        "id": "ouItprhidmfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features(example_batch):\n",
        "    input_encodings = tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n",
        "\n",
        "    return {\n",
        "        'input_ids' : input_encodings['input_ids'],\n",
        "        'attention_mask': input_encodings['attention_mask'],\n",
        "        'labels': target_encodings['input_ids']\n",
        "    }\n",
        "\n",
        "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)"
      ],
      "metadata": {
        "id": "FOCmgq3ZZbUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IQtV1nU_dvoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataCollatorForSeq2Seq"
      ],
      "metadata": {
        "id": "5F53mBnxdtoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)"
      ],
      "metadata": {
        "id": "78wMn_ruZbOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting the Training Arguments"
      ],
      "metadata": {
        "id": "wiKHqPg3dzuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "trainer_args = TrainingArguments(\n",
        "    output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,\n",
        "    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
        "    weight_decay=0.01, logging_steps=10,\n",
        "    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n",
        "    gradient_accumulation_steps=16\n",
        ")"
      ],
      "metadata": {
        "id": "6CHHpo5GZbLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing the Trainer Class"
      ],
      "metadata": {
        "id": "97dCA9d3d8JW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model=model_pegasus, args=training_args,\n",
        "                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
        "                  train_dataset=dataset_samsum_pt[\"train\"],\n",
        "                  eval_dataset=dataset_samsum_pt[\"validation\"])"
      ],
      "metadata": {
        "id": "q-YRFZPpZbIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the Model"
      ],
      "metadata": {
        "id": "Wrjn2hztd7uY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "\n",
        "score = calculate_metric_on_test_ds(\n",
        "    dataset_samsum['test'], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\n",
        ")\n",
        "\n",
        "rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
        "\n",
        "pd.DataFrame(rouge_dict, index = [f'pegasus'] )"
      ],
      "metadata": {
        "id": "XdOS0Wb6ZbFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the model and tokenizer"
      ],
      "metadata": {
        "id": "IDtG7Y_xeKPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_pegasus.save_pretrained(\"pegasus-samsum-model\")\n",
        "tokenizer.save_pretrained(\"tokenizer\")"
      ],
      "metadata": {
        "id": "v1zMPIgTeUE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the Fine-Tuned Model"
      ],
      "metadata": {
        "id": "hmhLIlOte86J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")\n",
        "sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n",
        "\n",
        "reference = dataset_samsum[\"test\"][0][\"summary\"]\n",
        "\n",
        "pipe = pipeline(\"summarization\", model=\"pegasus-samsum-model\",tokenizer=tokenizer )\n",
        "\n",
        "##\n",
        "print(\"Dialogue:\")\n",
        "print(sample_text)\n",
        "\n",
        "\n",
        "print(\"\\nReference Summary:\")\n",
        "print(reference)\n",
        "\n",
        "\n",
        "print(\"\\nModel Summary:\")\n",
        "print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"
      ],
      "metadata": {
        "id": "ISd8bS4-ZbDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "04qe9sxrZa__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vLssab6oZa9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IKqCTllzZa6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cefi2qJCZa3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G_ZP0rPcZa0j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}